{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_tabnet\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "import requests as r\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import pickle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Datensatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_data = pd.read_csv(\n",
    "    '/Users/michelbauer/Downloads/data_final(1).csv'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aufteilung in Trainings-, Validierung, und Testdatensatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "unique_game_ids = play_data['GameID'].unique()\n",
    "\n",
    "\n",
    "train_game_ids, remaining_game_ids = train_test_split(unique_game_ids, test_size=0.2, random_state=42)\n",
    "train_game_ids, valid_game_ids = train_test_split(train_game_ids, test_size=0.2, random_state=42)\n",
    "test_game_ids =remaining_game_ids\n",
    "\n",
    "# Aufteilen des play_data-Datensatzes basierend auf den GameIDs\n",
    "train_df = play_data[play_data['GameID'].isin(train_game_ids)]\n",
    "valid_df = play_data[play_data['GameID'].isin(valid_game_ids)]\n",
    "test_df = play_data[play_data['GameID'].isin(test_game_ids)]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_game_ids = set(train_df['GameID'])\n",
    "valid_game_ids = set(valid_df['GameID'])\n",
    "test_game_ids = set(test_df['GameID'])\n",
    "\n",
    "intersection_train_valid = train_game_ids.intersection(valid_game_ids)\n",
    "intersection_train_test = train_game_ids.intersection(test_game_ids)\n",
    "intersection_valid_test = valid_game_ids.intersection(test_game_ids)\n",
    "\n",
    "if len(intersection_train_valid) == 0 and len(intersection_train_test) == 0 and len(intersection_valid_test) == 0:\n",
    "    print(\"Keine gemeinsamen GameIDs zwischen train_df, valid_df und test_df.\")\n",
    "else:\n",
    "    print(\"Es gibt gemeinsame GameIDs zwischen train_df, valid_df und test_df.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import xgboost as xgb\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(eval_metric=['error', 'auc'], early_stopping_rounds=15)\n",
    "\n",
    "y_train_xgb = train_df['Winner']\n",
    "y_valid_xgb = valid_df['Winner']\n",
    "y_test_xgb = test_df['Winner']\n",
    "X_train_xgb = train_df.drop('Winner', axis=1)\n",
    "X_valid_xgb = valid_df.drop('Winner', axis=1)\n",
    "X_test_xgb = test_df.drop('Winner', axis=1)\n",
    "\n",
    "\n",
    "y_valid_xgb = y_valid_xgb.astype(int)\n",
    "y_train_xgb = y_train_xgb.astype(int)\n",
    "y_test_xgb = y_test_xgb.astype(int)\n",
    "cardinality_cols = ['GameID','type_text', 'team_play', 'start_down', 'end_down', 'season', 'week', 'Home_teamname', 'Away_teamname', 'leading_Team']\n",
    "numerical_cols = ['scoring_game','point_difference','awayScore', 'homeScore', 'scoreValue', 'statYardage', 'quarter', 'clock', 'start_distance', 'start_yardlinie', 'start_yardsToEndzone', 'end_distance', 'end_yardlinie', 'end_yardsToEndzone', 'elo_score_away', 'elo_score_home', 'attendence', 'time_remaining']\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_numerical_xgb = scaler.fit_transform(X_train_xgb[numerical_cols])\n",
    "X_valid_numerical_xgb = scaler.transform(X_valid_xgb[numerical_cols])\n",
    "X_test_numerical_xgb = scaler.transform(X_test_xgb[numerical_cols])\n",
    "\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "X_train_categorical_xgb = encoder.fit_transform(X_train_xgb[cardinality_cols]).toarray()\n",
    "X_valid_categorical_xgb = encoder.transform(X_valid_xgb[cardinality_cols]).toarray()\n",
    "X_test_categorical_xgb = encoder.transform(X_test_xgb[cardinality_cols]).toarray()\n",
    "\n",
    "\n",
    "X_train_processed_xgb = np.concatenate([X_train_numerical_xgb, X_train_categorical_xgb], axis=1)\n",
    "X_valid_processed_xgb = np.concatenate([X_valid_numerical_xgb, X_valid_categorical_xgb], axis=1)\n",
    "X_test_processed_xgb = np.concatenate([X_test_numerical_xgb, X_test_categorical_xgb], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "xgb_model.fit(\n",
    "    X=X_train_processed_xgb,\n",
    "    y=y_train_xgb,\n",
    "    eval_set=[(X_valid_processed_xgb, y_valid_xgb)],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "preds_test_xgb = xgb_model.predict(X_test_processed_xgb)\n",
    "\n",
    "\n",
    "accuracy_test_xgb = accuracy_score(y_test_xgb, preds_test_xgb)\n",
    "precision_test_xgb = precision_score(y_test_xgb, preds_test_xgb)\n",
    "recall_test_xgb = recall_score(y_test_xgb, preds_test_xgb)\n",
    "f1_test_xgb = f1_score(y_test_xgb, preds_test_xgb)\n",
    "auc_test_xgb = roc_auc_score(y_test_xgb, preds_test_xgb)\n",
    "cm_test_xgb = confusion_matrix(y_test_xgb, preds_test_xgb)\n",
    "cm_str_test_xgb = np.array2string(cm_test_xgb, separator=', ')\n",
    "\n",
    "\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "print(\"Accuracy:\", accuracy_test_xgb)\n",
    "print(\"Precision:\", precision_test_xgb)\n",
    "print(\"Recall:\", recall_test_xgb)\n",
    "print(\"F1 Score:\", f1_test_xgb)\n",
    "print(\"AUC:\", auc_test_xgb)\n",
    "print(\"Confusion Matrix:\\n\", cm_str_test_xgb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tn = TabNetClassifier(verbose=1, seed=42)\n",
    "\n",
    "y_train_tn = train_df['Winner']\n",
    "y_valid_tn = valid_df['Winner']\n",
    "y_test_tn = test_df['Winner']\n",
    "X_train_tn = train_df.drop('Winner', axis=1)\n",
    "X_valid_tn = valid_df.drop('Winner', axis=1)\n",
    "X_test_tn = test_df.drop('Winner', axis=1)\n",
    "\n",
    "y_valid_tn = y_valid_tn.astype(int)\n",
    "y_train_tn = y_train_tn.astype(int)\n",
    "y_test_tn = y_test_tn.astype(int)\n",
    "cardinality_cols = ['type_text', 'team_play', 'start_down', 'end_down', 'season', 'week', 'Home_teamname', 'Away_teamname', 'leading_Team']\n",
    "numerical_cols = ['GameID','scoring_game','point_difference','awayScore', 'homeScore', 'scoreValue', 'statYardage', 'quarter', 'clock', 'start_distance', 'start_yardlinie', 'start_yardsToEndzone', 'end_distance', 'end_yardlinie', 'end_yardsToEndzone', 'elo_score_away', 'elo_score_home', 'attendence', 'time_remaining']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_numerical_tn = scaler.fit_transform(X_train_tn[numerical_cols])\n",
    "X_valid_numerical_tn = scaler.transform(X_valid_tn[numerical_cols])\n",
    "X_test_numerical_tn = scaler.transform(X_test_tn[numerical_cols])\n",
    "\n",
    "\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "X_train_categorical_tn = encoder.fit_transform(X_train_tn[cardinality_cols]).toarray()\n",
    "X_valid_categorical_tn = encoder.transform(X_valid_tn[cardinality_cols]).toarray()\n",
    "X_test_categorical_tn = encoder.transform(X_test_tn[cardinality_cols]).toarray()\n",
    "\n",
    "\n",
    "X_train_processed_tn = np.concatenate([X_train_numerical_tn, X_train_categorical_tn], axis=1)\n",
    "X_valid_processed_tn = np.concatenate([X_valid_numerical_tn, X_valid_categorical_tn], axis=1)\n",
    "X_test_processed_tn = np.concatenate([X_test_numerical_tn, X_test_categorical_tn], axis=1)\n",
    "\n",
    "\n",
    "tn.fit(\n",
    "    X_train=X_train_processed_tn,\n",
    "    y_train=y_train_tn,\n",
    "    patience=30,\n",
    "    max_epochs=32,\n",
    "    eval_set=[(X_valid_processed_tn, y_valid_tn)],\n",
    "    eval_metric=['accuracy', 'auc'],\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "\n",
    "preds_test_tn = tn.predict(X_test_processed_tn)\n",
    "\n",
    "accuracy_test_tn = accuracy_score(y_test_tn, preds_test_tn)\n",
    "precision_test_tn = precision_score(y_test_tn, preds_test_tn)\n",
    "recall_test_tn = recall_score(y_test_tn, preds_test_tn)\n",
    "f1_test_tn = f1_score(y_test_tn, preds_test_tn)\n",
    "auc_test_tn = roc_auc_score(y_test_tn, preds_test_tn)\n",
    "cm_test_tn = confusion_matrix(y_test_tn, preds_test_tn)\n",
    "cm_str_test_tn = np.array2string(cm_test_tn, separator=', ')\n",
    "\n",
    "print(\"Test Accuracy: {:.2f}\".format(accuracy_test_tn))\n",
    "print(\"Test Precision: {:.2f}\".format(precision_test_tn))\n",
    "print(\"Test Recall: {:.2f}\".format(recall_test_tn))\n",
    "print(\"Test F1 Score: {:.2f}\".format(f1_test_tn))\n",
    "print('Test Confusion Matrix: ', cm_str_test_tn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "train_accs = []\n",
    "valid_accs = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "n_estimators = [10, 50, 100, 200, 300, 400, 500]\n",
    "y_train_rf = train_df['Winner']\n",
    "y_valid_rf = valid_df['Winner']\n",
    "y_test_rf = test_df['Winner']\n",
    "X_train_rf = train_df.drop('Winner', axis=1)\n",
    "X_valid_rf = valid_df.drop('Winner', axis=1)\n",
    "X_test_rf = test_df.drop('Winner', axis=1)\n",
    "\n",
    "\n",
    "y_valid_rf = y_valid_rf.astype(int)\n",
    "y_train_rf = y_train_rf.astype(int)\n",
    "y_test_rf = y_test_rf.astype(int)\n",
    "cardinality_cols = ['GameID','type_text', 'team_play', 'start_down', 'end_down', 'season', 'week', 'Home_teamname', 'Away_teamname', 'leading_Team']\n",
    "numerical_cols = ['scoring_game','point_difference','awayScore', 'homeScore', 'scoreValue', 'statYardage', 'quarter', 'clock', 'start_distance', 'start_yardlinie', 'start_yardsToEndzone', 'end_distance', 'end_yardlinie', 'end_yardsToEndzone', 'elo_score_away', 'elo_score_home', 'attendence', 'time_remaining']\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_numerical_rf = scaler.fit_transform(X_train_rf[numerical_cols])\n",
    "X_valid_numerical_rf = scaler.transform(X_valid_rf[numerical_cols])\n",
    "X_test_numerical_rf = scaler.transform(X_test_rf[numerical_cols])\n",
    "\n",
    "\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "X_train_categorical_rf = encoder.fit_transform(X_train_rf[cardinality_cols]).toarray()\n",
    "X_valid_categorical_rf = encoder.transform(X_valid_rf[cardinality_cols]).toarray()\n",
    "X_test_categorical_rf = encoder.transform(X_test_rf[cardinality_cols]).toarray()\n",
    "\n",
    "\n",
    "X_train_processed_rf = np.concatenate([X_train_numerical_rf, X_train_categorical_rf], axis=1)\n",
    "X_valid_processed_rf = np.concatenate([X_valid_numerical_rf, X_valid_categorical_rf], axis=1)\n",
    "X_test_processed_rf = np.concatenate([X_test_numerical_rf, X_test_categorical_rf], axis=1)\n",
    "\n",
    "\n",
    "for n in n_estimators:\n",
    "    \n",
    "    rf_model = RandomForestClassifier(n_estimators=n, random_state=0)\n",
    "    rf_model.fit(X_train_processed_rf, y_train_rf)\n",
    "\n",
    "    \n",
    "    preds_train_rf = rf_model.predict(X_train_processed_rf)\n",
    "    preds_valid_rf = rf_model.predict(X_valid_processed_rf)\n",
    "\n",
    "    \n",
    "    train_acc = accuracy_score(y_train_rf, preds_train_rf)\n",
    "    valid_acc = accuracy_score(y_valid_rf, preds_valid_rf)\n",
    "\n",
    "    \n",
    "    train_accs.append(train_acc)\n",
    "    valid_accs.append(valid_acc)\n",
    "\n",
    "\n",
    "plt.plot(n_estimators, train_accs, label='Training Accuracy')\n",
    "plt.plot(n_estimators, valid_accs, label='Validation Accuracy')\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Validation Curve: Random Forest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "\n",
    "y_train_rf = train_df['Winner']\n",
    "y_valid_rf = valid_df['Winner']\n",
    "y_test_rf = test_df['Winner']\n",
    "X_train_rf = train_df.drop('Winner', axis=1)\n",
    "X_valid_rf = valid_df.drop('Winner', axis=1)\n",
    "X_test_rf = test_df.drop('Winner', axis=1)\n",
    "\n",
    "\n",
    "y_valid_rf = y_valid_rf.astype(int)\n",
    "y_train_rf = y_train_rf.astype(int)\n",
    "y_test_rf = y_test_rf.astype(int)\n",
    "\n",
    "\n",
    "cardinality_cols = ['GameID','type_text', 'team_play', 'start_down', 'end_down', 'season', 'week', 'Home_teamname', 'Away_teamname', 'leading_Team']\n",
    "numerical_cols = ['scoring_game','point_difference','awayScore', 'homeScore', 'scoreValue', 'statYardage', 'quarter', 'clock', 'start_distance', 'start_yardlinie', 'start_yardsToEndzone', 'end_distance', 'end_yardlinie', 'end_yardsToEndzone', 'elo_score_away', 'elo_score_home', 'attendence', 'time_remaining']\n",
    "\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_numerical_rf = scaler.fit_transform(X_train_rf[numerical_cols])\n",
    "X_valid_numerical_rf = scaler.transform(X_valid_rf[numerical_cols])\n",
    "X_test_numerical_rf = scaler.transform(X_test_rf[numerical_cols])\n",
    "\n",
    "\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "X_train_categorical_rf = encoder.fit_transform(X_train_rf[cardinality_cols]).toarray()\n",
    "X_valid_categorical_rf = encoder.transform(X_valid_rf[cardinality_cols]).toarray()\n",
    "X_test_categorical_rf = encoder.transform(X_test_rf[cardinality_cols]).toarray()\n",
    "\n",
    "\n",
    "X_train_processed_rf = np.concatenate([X_train_numerical_rf, X_train_categorical_rf], axis=1)\n",
    "X_valid_processed_rf = np.concatenate([X_valid_numerical_rf, X_valid_categorical_rf], axis=1)\n",
    "X_test_processed_rf = np.concatenate([X_test_numerical_rf, X_test_categorical_rf], axis=1)\n",
    "\n",
    "\n",
    "rf_model.fit(X=X_train_processed_rf,\n",
    "              y=y_train_rf)\n",
    "\n",
    "\n",
    "preds_test_rf = rf_model.predict(X_test_processed_rf)\n",
    "\n",
    "\n",
    "accuracy_test_rf = accuracy_score(y_test_rf, preds_test_rf)\n",
    "precision_test_rf = precision_score(y_test_rf, preds_test_rf)\n",
    "recall_test_rf = recall_score(y_test_rf, preds_test_rf)\n",
    "f1_test_rf = f1_score(y_test_rf, preds_test_rf)\n",
    "auc_test_rf = roc_auc_score(y_test_rf, preds_test_rf)\n",
    "cm_test_rf = confusion_matrix(y_test_rf, preds_test_rf)\n",
    "cm_str_test_rf = np.array2string(cm_test_rf, separator=', ')\n",
    "\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "print(\"Accuracy:\", accuracy_test_rf)\n",
    "print(\"Precision:\", precision_test_rf)\n",
    "print(\"Recall:\", recall_test_rf)\n",
    "print(\"F1 Score:\", f1_test_rf)\n",
    "print(\"AUC:\", auc_test_rf)\n",
    "print(\"Confusion Matrix:\\n\", cm_str_test_rf)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
